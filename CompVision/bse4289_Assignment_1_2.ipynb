{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN5lM2csj9v7"
      },
      "source": [
        "# Computer Vision Assignment 1 Part 2\n",
        "---\n",
        "\n",
        "Semester: **Fall 2022**\n",
        "\n",
        "Due date: **September 29th 2022, 11.59PM EST.**\n",
        "\n",
        "## Introduction\n",
        "---\n",
        "This assignment requires you to participate in a Kaggle competition with the rest of the class on the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb). The objective is to produce a model that gives the highest possible accuracy on the test portion of this dataset. You can register for the competition using the private link: https://www.kaggle.com/c/nyu-computer-vision-csci-ga2271-2022/overview.\n",
        "\n",
        "Skeleton code is provided in the colab below. This contains code for training a simple default model and evaluating it on the test set. The evaluation script produces a file `gtsrb_kaggle.csv` that lists the IDs of the test set images, along with their predicted label. This file should be uploaded to the Kaggle webpage, which will then produce a test accuracy score. \n",
        "\n",
        "Your goal is to implement a new model architecture that improves upon the baseline performance. You are free to implement any approach covered in class or from research papers. This part will count for 50% of the overall grade for assignment 1. This Grading will depend on your Kaggle performance and rank, as well as novelty of the architecture.  \n",
        "\n",
        "## Rules\n",
        "---\n",
        "You should make a copy of this Colab (`File->Save a copy in Drive`). Please start the assignment early and don’t be afraid to ask for help from either the TAs or myself. You are allowed to collaborate with other students in terms discussing ideas and possible solutions. However you code up the solution yourself, i.e. you must write your own code. Copying your friends code and just changing all the names of the variables is NOT ALLOWED! You are not allowed to use solutions from similar assignments in courses from other institutions, or those found elsewhere on the web.\n",
        "\n",
        "Your solutions should be submitted via the Brightspace system. This should include a brief description (in the Colab) explaining the model architectures you explored, citing any relevant papers or techniques that you used. You should also include convergence plots of training accuracy vs epoch for relevant models. \n",
        "\n",
        "## Important Details\n",
        "---\n",
        "• You are only allowed 8 submissions to the Kaggle evaluation server per day. This is to prevent over-fitting on the test dataset. So be sure to start the assignment early!\n",
        "\n",
        "• You are NOT ALLOWED to use the test set labels during training in any way. Doing so will be regarded as cheating and penalized accordingly.\n",
        "\n",
        "• The evaluation metric is accuracy, i.e. the fraction of test set examples where the predicted label agrees with the ground truth label.\n",
        "\n",
        "• You should be able to achieve a test accuracy of at least 95% \n",
        "\n",
        "• **Extra important:** Please use your NYU NetID as your team name on Kaggle, so the TAs can figure out which user you are on the leaderboard. \n",
        "\n",
        "# Dataset Preparation\n",
        "___\n",
        "\n",
        "1.  Download `dataset.zip` from the course website to your local machine.\n",
        "2.  Unzip the file. You should see a `dataset` directory with three subfolders: `training`, `validation`, and `testing`. \n",
        "3.  Go to Google Drive (on your NYU account) and make a new directory (say `cv_kaggle_assignment`).\n",
        "4.  Upload each of the three subfolders to it. \n",
        "5.  Run the code block below. It will ask for permission to mount your Google Drive (NYU account) so this colab can access it. Paste the authorization code into the box as requested. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had trouble getting this model to converge, not sure what i was doing wrong in the resnet"
      ],
      "metadata": {
        "id": "zmTVQPfAshcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0aPnIKXpWbN",
        "outputId": "008cb1dd-e602-44eb-f79e-4522e04da39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/cv_kaggle_assignment\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/cv_kaggle_assignment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6jVfIVtrn5u"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "z21UKj_bT--_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 32\n",
        "momentum = 0.9\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "log_interval = 100\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_path=\"X.pt\", y_path=\"y.pt\"):\n",
        "\n",
        "        self.X = torch.load(X_path).squeeze(1)\n",
        "        self.y = torch.load(y_path).squeeze(1)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = MyDataset(X_path=\"train/X.pt\", y_path=\"train/y.pt\")\n",
        "val_dataset = MyDataset(X_path=\"validation/X.pt\", y_path=\"validation/y.pt\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6W0pQRvZKO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "zeev4SoMvazV"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nclasses = 43 # GTSRB has 43 classes\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,input,output,downsample):\n",
        "        super().__init__()\n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(input, output, kernel_size=3, stride=2, padding=1) #can mess with the parameters later\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(input, output, kernel_size=1, stride=2),\n",
        "                nn.BatchNorm2d(output)\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1)\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Identity()\n",
        "            )\n",
        "\n",
        "        self.conv2 = nn.Conv2d(output, output, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(output)\n",
        "        self.bn2 = nn.BatchNorm2d(output)\n",
        "\n",
        "    def forward(self, input):\n",
        "        skip_layer = self.skip(input)\n",
        "        input = F.relu(self.bn1(self.conv1(input)))\n",
        "        input = F.relu(self.bn2(self.conv2(input)))\n",
        "        input = input + skip_layer\n",
        "        return F.relu(input)\n",
        "\n",
        "class ResBottleneckBlock(nn.Module):\n",
        "    def __init__(self, input, output, downsample):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(input, output//4, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(output//4, output//4, kernel_size=3, stride=2 if downsample else 1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(output//4, output, kernel_size=1, stride=1)\n",
        "\n",
        "        if self.downsample or input != output:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(input, output, kernel_size=1, stride=2 if self.downsample else 1),\n",
        "                nn.BatchNorm2d(output)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(output//4)\n",
        "        self.bn2 = nn.BatchNorm2d(output//4)\n",
        "        self.bn3 = nn.BatchNorm2d(output)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shortcut = self.shortcut(input)\n",
        "        input = F.relu(self.bn1(self.conv1(input)))\n",
        "        input = F.relu(self.bn2(self.conv2(input)))\n",
        "        input = F.relu(self.bn3(self.conv3(input)))\n",
        "        input = input + shortcut\n",
        "        return F.relu(input)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, input, resblock, repeat, useBottleneck=False, final_out = nclasses):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Conv2d(input,64, 7, 2, 3)\n",
        "        self.bn0 = nn.BatchNorm2d(64)\n",
        "\n",
        "        if useBottleneck:\n",
        "            filters = [64,256,512,1024,2048]\n",
        "        else:\n",
        "            filters = [64,64,128,256,512]\n",
        "\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n",
        "        for i in range(1, repeat[0]):\n",
        "            self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n",
        "\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n",
        "        for i in range(1, repeat[1]):\n",
        "            self.layer2.add_module('conv3_%d'%(i+1,), resblock(filters[2], filters[2], downsample=False))\n",
        "\n",
        "        self.layer3 = nn.Sequential()\n",
        "        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n",
        "        for i in range(1, repeat[2]):\n",
        "            self.layer3.add_module('conv2_%d'%(i+1,), resblock(filters[3], filters[3], downsample=False))\n",
        "\n",
        "        self.layer4 = nn.Sequential()\n",
        "        self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n",
        "        for i in range(1, repeat[3]):\n",
        "            self.layer4.add_module('conv3_%d'%(i+1,), resblock(filters[4], filters[4], downsample=False))\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(filters[4],final_out)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = F.relu(self.bn0(F.max_pool2d(self.conv0(input),2)))\n",
        "        input = self.layer1(input)\n",
        "        input = self.layer2(input)\n",
        "        input = self.layer3(input)\n",
        "        input = self.layer4(input)\n",
        "        input = self.gap(input)\n",
        "        input = torch.flatten(input,1)\n",
        "        input = self.fc(input)\n",
        "\n",
        "        return input\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkeletonNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkeletonNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(500, 50)\n",
        "        self.fc2 = nn.Linear(50, nclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 500)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=1)"
      ],
      "metadata": {
        "id": "hGwEWwvarR5l"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NewerNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=7)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=7)\n",
        "        self.conv3 = nn.Conv2d(32,64,kernel_size = 3,padding=1)\n",
        "        self.conv4 = nn.Conv2d(64,64,kernel_size = 3,padding=1)\n",
        "        self.fc1 = nn.Linear(64, 50)\n",
        "        self.fc2 = nn.Linear(50, nclasses)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(self.bn(self.conv3(x)))\n",
        "        x_skip = x\n",
        "        x = F.relu(self.bn(self.conv4(x)))\n",
        "        x = F.relu(self.bn(self.conv4(x)))\n",
        "        x = F.relu(x_skip+x)\n",
        "        x_skip = x\n",
        "        x = F.relu(self.bn(self.conv4(x)))\n",
        "        x = F.relu(self.bn(self.conv4(x)))\n",
        "        x = F.relu(x_skip+x)\n",
        "        x = F.relu(F.max_pool2d(self.conv4(x), 2))\n",
        "        x = torch.flatten(x,1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=1)"
      ],
      "metadata": {
        "id": "HMW0P1buMw7t"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NewNet, self).__init__()\n",
        "        #Layer 0\n",
        "        self.conv_0 = nn.Conv2d(3,64,kernel_size=7, stride = 2)\n",
        "        self.bn_0 = nn.BatchNorm2d(64)\n",
        "        #Layer 1\n",
        "        self.conv_1 = nn.Conv2d(64,64,kernel_size=3)\n",
        "        self.bn_1 = nn.BatchNorm2d(64)\n",
        "        #Layer 2\n",
        "        self.conv_2_s = nn.Conv2d(64,128,kernel_size=1,stride=2)\n",
        "        self.conv_2_e = nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_2 = nn.Conv2d(128,128,kernel_size=3,padding=1)\n",
        "        self.bn_2 = nn.BatchNorm2d(128)\n",
        "        #Layer 3\n",
        "        self.conv_3_s = nn.Conv2d(128,256,kernel_size=1,stride=2)\n",
        "        self.conv_3_e = nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_3 = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
        "        self.bn_3 = nn.BatchNorm2d(256)\n",
        "        #Layer 4\n",
        "        self.conv_4_s = nn.Conv2d(256,512,kernel_size=1,stride=2)\n",
        "        self.conv_4_e = nn.Conv2d(256,512,kernel_size=3,stride=2,padding=1)\n",
        "        self.conv_4 = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.bn_4 = nn.BatchNorm2d(512)\n",
        "        #Final Layers\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512,nclasses)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Layer 0\n",
        "        x = F.relu(self.bn_0(F.max_pool2d(self.conv_0(x), 2)))\n",
        "\n",
        "        #Layer 1\n",
        "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
        "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
        "        \n",
        "        #Layer 2\n",
        "        x_skip = self.conv_2_s(x)\n",
        "        x = F.relu(self.bn_2(self.conv_2_e(x)))\n",
        "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
        "        x = F.relu(x_skip + x)\n",
        "        x_skip = x\n",
        "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
        "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
        "        x = F.relu(x_skip + x)\n",
        "\n",
        "        #Layer 3\n",
        "        x_skip = self.conv_3_s(x)\n",
        "        x = F.relu(self.bn_3(self.conv_3_e(x)))\n",
        "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
        "        x = F.relu(x_skip + x)\n",
        "        x_skip = x\n",
        "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
        "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
        "        x = F.relu(x_skip + x)\n",
        "\n",
        "        #Layer 4\n",
        "        x_skip = self.conv_4_s(x)\n",
        "        x = F.relu(self.bn_4(self.conv_4_e(x)))\n",
        "        x = F.relu(self.bn_4(self.conv_4(x)))\n",
        "        x = F.relu(x_skip + x)\n",
        "        x_skip = x\n",
        "        x = F.relu(self.bn_4(self.conv_4(x)))\n",
        "        x = F.relu(self.bn_4(self.conv_4(x)))\n",
        "        x = F.relu(x_skip+x)\n",
        "\n",
        "        #Final\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "KtM7xqVexqj1"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty9TAvrdvf8C"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A5-OCgBvhXv",
        "outputId": "1e6207e7-4db4-44a7-a8b2-b784454b072c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/35339 (0%)]\tLoss: 3.919750\n",
            "Train Epoch: 1 [3200/35339 (9%)]\tLoss: 1.405856\n",
            "Train Epoch: 1 [6400/35339 (18%)]\tLoss: 0.785179\n",
            "Train Epoch: 1 [9600/35339 (27%)]\tLoss: 0.634782\n",
            "Train Epoch: 1 [12800/35339 (36%)]\tLoss: 0.407366\n",
            "Train Epoch: 1 [16000/35339 (45%)]\tLoss: 0.685456\n",
            "Train Epoch: 1 [19200/35339 (54%)]\tLoss: 0.458763\n",
            "Train Epoch: 1 [22400/35339 (63%)]\tLoss: 0.235052\n",
            "Train Epoch: 1 [25600/35339 (72%)]\tLoss: 0.503960\n",
            "Train Epoch: 1 [28800/35339 (81%)]\tLoss: 0.658624\n",
            "Train Epoch: 1 [32000/35339 (90%)]\tLoss: 0.307448\n",
            "Train Epoch: 1 [35200/35339 (100%)]\tLoss: 0.298117\n",
            "\n",
            "Validation set: Average loss: 0.7433, Accuracy: 3077/3870 (80%)\n",
            "\n",
            "\n",
            "Saved model to model_1.pth.\n",
            "Train Epoch: 2 [0/35339 (0%)]\tLoss: 0.032388\n",
            "Train Epoch: 2 [3200/35339 (9%)]\tLoss: 0.198258\n",
            "Train Epoch: 2 [6400/35339 (18%)]\tLoss: 0.066545\n",
            "Train Epoch: 2 [9600/35339 (27%)]\tLoss: 0.139531\n",
            "Train Epoch: 2 [12800/35339 (36%)]\tLoss: 0.046067\n",
            "Train Epoch: 2 [16000/35339 (45%)]\tLoss: 0.220055\n",
            "Train Epoch: 2 [19200/35339 (54%)]\tLoss: 0.105442\n",
            "Train Epoch: 2 [22400/35339 (63%)]\tLoss: 0.062842\n",
            "Train Epoch: 2 [25600/35339 (72%)]\tLoss: 0.163349\n",
            "Train Epoch: 2 [28800/35339 (81%)]\tLoss: 0.065672\n",
            "Train Epoch: 2 [32000/35339 (90%)]\tLoss: 0.024116\n",
            "Train Epoch: 2 [35200/35339 (100%)]\tLoss: 0.058287\n",
            "\n",
            "Validation set: Average loss: 0.7218, Accuracy: 3081/3870 (80%)\n",
            "\n",
            "\n",
            "Saved model to model_2.pth.\n",
            "Train Epoch: 3 [0/35339 (0%)]\tLoss: 0.007004\n",
            "Train Epoch: 3 [3200/35339 (9%)]\tLoss: 0.288029\n",
            "Train Epoch: 3 [6400/35339 (18%)]\tLoss: 0.016392\n",
            "Train Epoch: 3 [9600/35339 (27%)]\tLoss: 0.055261\n",
            "Train Epoch: 3 [12800/35339 (36%)]\tLoss: 0.006833\n",
            "Train Epoch: 3 [16000/35339 (45%)]\tLoss: 0.094699\n",
            "Train Epoch: 3 [19200/35339 (54%)]\tLoss: 0.129854\n",
            "Train Epoch: 3 [22400/35339 (63%)]\tLoss: 0.003809\n",
            "Train Epoch: 3 [25600/35339 (72%)]\tLoss: 0.020960\n",
            "Train Epoch: 3 [28800/35339 (81%)]\tLoss: 0.167689\n",
            "Train Epoch: 3 [32000/35339 (90%)]\tLoss: 0.020935\n",
            "Train Epoch: 3 [35200/35339 (100%)]\tLoss: 0.075456\n",
            "\n",
            "Validation set: Average loss: 0.7475, Accuracy: 3042/3870 (79%)\n",
            "\n",
            "\n",
            "Saved model to model_3.pth.\n",
            "Train Epoch: 4 [0/35339 (0%)]\tLoss: 0.012024\n",
            "Train Epoch: 4 [3200/35339 (9%)]\tLoss: 0.096461\n",
            "Train Epoch: 4 [6400/35339 (18%)]\tLoss: 0.181256\n",
            "Train Epoch: 4 [9600/35339 (27%)]\tLoss: 0.043832\n",
            "Train Epoch: 4 [12800/35339 (36%)]\tLoss: 0.036685\n",
            "Train Epoch: 4 [16000/35339 (45%)]\tLoss: 0.002776\n",
            "Train Epoch: 4 [19200/35339 (54%)]\tLoss: 0.002119\n",
            "Train Epoch: 4 [22400/35339 (63%)]\tLoss: 0.115872\n",
            "Train Epoch: 4 [25600/35339 (72%)]\tLoss: 0.006409\n",
            "Train Epoch: 4 [28800/35339 (81%)]\tLoss: 0.160246\n",
            "Train Epoch: 4 [32000/35339 (90%)]\tLoss: 0.082039\n",
            "Train Epoch: 4 [35200/35339 (100%)]\tLoss: 0.073426\n",
            "\n",
            "Validation set: Average loss: 0.6348, Accuracy: 3204/3870 (83%)\n",
            "\n",
            "\n",
            "Saved model to model_4.pth.\n",
            "Train Epoch: 5 [0/35339 (0%)]\tLoss: 0.013030\n",
            "Train Epoch: 5 [3200/35339 (9%)]\tLoss: 0.025859\n",
            "Train Epoch: 5 [6400/35339 (18%)]\tLoss: 0.000308\n",
            "Train Epoch: 5 [9600/35339 (27%)]\tLoss: 0.006287\n",
            "Train Epoch: 5 [12800/35339 (36%)]\tLoss: 0.001349\n",
            "Train Epoch: 5 [16000/35339 (45%)]\tLoss: 0.003250\n",
            "Train Epoch: 5 [19200/35339 (54%)]\tLoss: 0.108442\n",
            "Train Epoch: 5 [22400/35339 (63%)]\tLoss: 0.069784\n",
            "Train Epoch: 5 [25600/35339 (72%)]\tLoss: 0.003696\n",
            "Train Epoch: 5 [28800/35339 (81%)]\tLoss: 0.074318\n",
            "Train Epoch: 5 [32000/35339 (90%)]\tLoss: 0.015338\n",
            "Train Epoch: 5 [35200/35339 (100%)]\tLoss: 0.008272\n",
            "\n",
            "Validation set: Average loss: 0.7148, Accuracy: 3224/3870 (83%)\n",
            "\n",
            "\n",
            "Saved model to model_5.pth.\n",
            "Train Epoch: 6 [0/35339 (0%)]\tLoss: 0.032292\n",
            "Train Epoch: 6 [3200/35339 (9%)]\tLoss: 0.000442\n",
            "Train Epoch: 6 [6400/35339 (18%)]\tLoss: 0.005480\n",
            "Train Epoch: 6 [9600/35339 (27%)]\tLoss: 0.006425\n",
            "Train Epoch: 6 [12800/35339 (36%)]\tLoss: 0.002953\n",
            "Train Epoch: 6 [16000/35339 (45%)]\tLoss: 0.069067\n",
            "Train Epoch: 6 [19200/35339 (54%)]\tLoss: 0.003954\n",
            "Train Epoch: 6 [22400/35339 (63%)]\tLoss: 0.000042\n",
            "Train Epoch: 6 [25600/35339 (72%)]\tLoss: 0.023972\n",
            "Train Epoch: 6 [28800/35339 (81%)]\tLoss: 0.001493\n",
            "Train Epoch: 6 [32000/35339 (90%)]\tLoss: 0.001137\n",
            "Train Epoch: 6 [35200/35339 (100%)]\tLoss: 0.026966\n",
            "\n",
            "Validation set: Average loss: 0.8014, Accuracy: 3160/3870 (82%)\n",
            "\n",
            "\n",
            "Saved model to model_6.pth.\n",
            "Train Epoch: 7 [0/35339 (0%)]\tLoss: 0.009675\n",
            "Train Epoch: 7 [3200/35339 (9%)]\tLoss: 0.002017\n",
            "Train Epoch: 7 [6400/35339 (18%)]\tLoss: 0.157152\n",
            "Train Epoch: 7 [9600/35339 (27%)]\tLoss: 0.024177\n",
            "Train Epoch: 7 [12800/35339 (36%)]\tLoss: 0.038291\n",
            "Train Epoch: 7 [16000/35339 (45%)]\tLoss: 0.004262\n",
            "Train Epoch: 7 [19200/35339 (54%)]\tLoss: 0.000244\n",
            "Train Epoch: 7 [22400/35339 (63%)]\tLoss: 0.000275\n",
            "Train Epoch: 7 [25600/35339 (72%)]\tLoss: 0.095143\n",
            "Train Epoch: 7 [28800/35339 (81%)]\tLoss: 0.036779\n",
            "Train Epoch: 7 [32000/35339 (90%)]\tLoss: 0.001717\n",
            "Train Epoch: 7 [35200/35339 (100%)]\tLoss: 0.167528\n",
            "\n",
            "Validation set: Average loss: 0.7538, Accuracy: 3149/3870 (81%)\n",
            "\n",
            "\n",
            "Saved model to model_7.pth.\n",
            "Train Epoch: 8 [0/35339 (0%)]\tLoss: 0.091228\n",
            "Train Epoch: 8 [3200/35339 (9%)]\tLoss: 0.003446\n",
            "Train Epoch: 8 [6400/35339 (18%)]\tLoss: 0.005278\n",
            "Train Epoch: 8 [9600/35339 (27%)]\tLoss: 0.001223\n",
            "Train Epoch: 8 [12800/35339 (36%)]\tLoss: 0.000026\n",
            "Train Epoch: 8 [16000/35339 (45%)]\tLoss: 0.006649\n",
            "Train Epoch: 8 [19200/35339 (54%)]\tLoss: 0.001729\n",
            "Train Epoch: 8 [22400/35339 (63%)]\tLoss: 0.019934\n",
            "Train Epoch: 8 [25600/35339 (72%)]\tLoss: 0.023101\n",
            "Train Epoch: 8 [28800/35339 (81%)]\tLoss: 0.000086\n",
            "Train Epoch: 8 [32000/35339 (90%)]\tLoss: 0.001468\n",
            "Train Epoch: 8 [35200/35339 (100%)]\tLoss: 0.027084\n",
            "\n",
            "Validation set: Average loss: 0.5430, Accuracy: 3379/3870 (87%)\n",
            "\n",
            "\n",
            "Saved model to model_8.pth.\n",
            "Train Epoch: 9 [0/35339 (0%)]\tLoss: 0.029487\n",
            "Train Epoch: 9 [3200/35339 (9%)]\tLoss: 0.004198\n",
            "Train Epoch: 9 [6400/35339 (18%)]\tLoss: 0.000186\n",
            "Train Epoch: 9 [9600/35339 (27%)]\tLoss: 0.000169\n",
            "Train Epoch: 9 [12800/35339 (36%)]\tLoss: 0.001874\n",
            "Train Epoch: 9 [16000/35339 (45%)]\tLoss: 0.044778\n",
            "Train Epoch: 9 [19200/35339 (54%)]\tLoss: 0.003532\n",
            "Train Epoch: 9 [22400/35339 (63%)]\tLoss: 0.011925\n",
            "Train Epoch: 9 [25600/35339 (72%)]\tLoss: 0.010813\n",
            "Train Epoch: 9 [28800/35339 (81%)]\tLoss: 0.007927\n",
            "Train Epoch: 9 [32000/35339 (90%)]\tLoss: 0.000087\n",
            "Train Epoch: 9 [35200/35339 (100%)]\tLoss: 0.029432\n",
            "\n",
            "Validation set: Average loss: 0.6373, Accuracy: 3257/3870 (84%)\n",
            "\n",
            "\n",
            "Saved model to model_9.pth.\n",
            "Train Epoch: 10 [0/35339 (0%)]\tLoss: 0.000173\n",
            "Train Epoch: 10 [3200/35339 (9%)]\tLoss: 0.000059\n",
            "Train Epoch: 10 [6400/35339 (18%)]\tLoss: 0.004636\n",
            "Train Epoch: 10 [9600/35339 (27%)]\tLoss: 0.006125\n",
            "Train Epoch: 10 [12800/35339 (36%)]\tLoss: 0.002032\n",
            "Train Epoch: 10 [16000/35339 (45%)]\tLoss: 0.000285\n",
            "Train Epoch: 10 [19200/35339 (54%)]\tLoss: 0.000249\n",
            "Train Epoch: 10 [22400/35339 (63%)]\tLoss: 0.001272\n",
            "Train Epoch: 10 [25600/35339 (72%)]\tLoss: 0.063553\n",
            "Train Epoch: 10 [28800/35339 (81%)]\tLoss: 0.051008\n",
            "Train Epoch: 10 [32000/35339 (90%)]\tLoss: 0.003688\n",
            "Train Epoch: 10 [35200/35339 (100%)]\tLoss: 0.001491\n",
            "\n",
            "Validation set: Average loss: 0.5996, Accuracy: 3271/3870 (85%)\n",
            "\n",
            "\n",
            "Saved model to model_10.pth.\n"
          ]
        }
      ],
      "source": [
        "#model = ResNet(3, ResBottleneckBlock, [3,4,6,3], useBottleneck=False, final_out=nclasses)\n",
        "model = NewerNet()\n",
        "#model = SkeletonNet()\n",
        "lr = .01\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return loss\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        output = model(data)\n",
        "        validation_loss += F.nll_loss(output, target, reduction=\"sum\").item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train(epoch)\n",
        "    validation()\n",
        "    model_file = 'model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN1f1p7w59X"
      },
      "source": [
        "# Evaluate and Submit to Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BM5qP64w5zB",
        "outputId": "72170ec0-b9dd-44a2-aa2d-d6a738cac39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written to csv file gtsrb_kaggle.csv\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "outfile = 'gtsrb_kaggle.csv'\n",
        "\n",
        "output_file = open(outfile, \"w\")\n",
        "dataframe_dict = {\"Filename\" : [], \"ClassId\": []}\n",
        "\n",
        "test_data = torch.load('testing/test.pt')\n",
        "file_ids = pickle.load(open('testing/file_ids.pkl', 'rb'))\n",
        "model.eval() # Don't forget to put your model on eval mode !\n",
        "\n",
        "for i, data in enumerate(test_data):\n",
        "    data = data.unsqueeze(0)\n",
        "\n",
        "    output = model(data)\n",
        "    pred = output.data.max(1, keepdim=True)[1].item()\n",
        "    file_id = file_ids[i][0:5]\n",
        "    dataframe_dict['Filename'].append(file_id)\n",
        "    dataframe_dict['ClassId'].append(pred)\n",
        "\n",
        "df = pd.DataFrame(data=dataframe_dict)\n",
        "df.to_csv(outfile, index=False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSl_4kn6sox"
      },
      "source": [
        "# Submitting to Kaggle\n",
        "\n",
        "Now download the CSV file `grtsrb_kaggle.csv` from your Google drive and then submit it to Kaggle to check the performance of your model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}